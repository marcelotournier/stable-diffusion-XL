{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPs1GCS/z8Y9TpQTkdNVTvA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcelotournier/stable-diffusion-XL/blob/main/01_treinar_stable_diffusion_xl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treinar o Stable Diffusion XL\n",
        "\n",
        "Esse notebook faz o treino (fine-tuning) do modelo Stable Diffusion XL para gerar imagens personalizadas de pessoas, criaturas ou objetos (Dreambooth).\n",
        "\n",
        "### Instruções:\n",
        "\n",
        "1. Crie uma pasta `images` no colab\n",
        "2. Faça o upload de 4 imagens em formato jpg para a pasta `images`. Tente escolher imagens da pessoa, criatura ou objeto que você deseja em diferentes poses, de preferência apenas a pessoa/criatura/objeto aparecendo\n",
        "3. Modifique o `--prompt` caso você não esteja criando fotos de pessoas. Ex: `photo of sks cat` para um gato ou `photo of sks car` para um carro\n",
        "4. **IMPORTANTE:** Modifique o Runtime/Ambiente de execução do notebook para GPU antes de rodar\n",
        "5. Rode todas as linhas abaixo. O treino leva uns 20-40 minutos, dependendo da quantidade de imagens usadas\n",
        "6. Não se esqueça de fazer o download do seu modelo quando o treino do StableDiffusion terminar! O modelo está na pasta `output`. Baixe o arquivo `pytorch_lora_weights.safetensors` para seu computador local\n",
        "7. Vamos gerar as imagens usando o notebook no link abaixo:\n"
      ],
      "metadata": {
        "id": "OoAurrzCQIz-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ETUuqWeN4GS"
      },
      "outputs": [],
      "source": [
        "!pip install -U autotrain-advanced --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!autotrain setup --update-torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwDh779rOK3O",
        "outputId": "33182f7e-284f-45dd-f605-f824be2ce7c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-31 14:48:52.864099: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator9allocatorE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n",
            "> \u001b[1mINFO    Installing latest transformers@main\u001b[0m\n",
            "> \u001b[1mINFO    Successfully installed latest transformers\u001b[0m\n",
            "> \u001b[1mINFO    Installing latest peft@main\u001b[0m\n",
            "> \u001b[1mINFO    Successfully installed latest peft\u001b[0m\n",
            "> \u001b[1mINFO    Installing latest diffusers@main\u001b[0m\n",
            "> \u001b[1mINFO    Successfully installed latest diffusers\u001b[0m\n",
            "> \u001b[1mINFO    Installing latest trl@main\u001b[0m\n",
            "> \u001b[1mINFO    Successfully installed latest trl\u001b[0m\n",
            "> \u001b[1mINFO    Installing latest PyTorch\u001b[0m\n",
            "> \u001b[1mINFO    Successfully installed latest PyTorch\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!autotrain dreambooth \\\n",
        "--model stabilityai/stable-diffusion-xl-base-1.0 \\\n",
        "--output 'output/' \\\n",
        "--image-path 'images/' \\\n",
        "--prompt \"photo of sks person\" \\\n",
        "--resolution 512 \\\n",
        "--batch-size 1 \\\n",
        "--num-steps 500 \\\n",
        "--fp16 \\\n",
        "--gradient-accumulation 4 \\\n",
        "--lr 1e-4 \\\n",
        "--checkpointing-steps 100000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eDIeY-9OMFN",
        "outputId": "6a8cb83e-780d-4fdd-bcc3-753668404925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-31 14:55:24.757957: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "> \u001b[1mINFO    Namespace(version=False, model='stabilityai/stable-diffusion-xl-base-1.0', revision=None, tokenizer=None, image_path='images/', class_image_path=None, prompt='photo of sks person', class_prompt=None, num_class_images=100, class_labels_conditioning=None, prior_preservation=None, prior_loss_weight=1.0, output='output/', seed=42, resolution=512, center_crop=None, train_text_encoder=None, batch_size=1, sample_batch_size=4, epochs=1, num_steps=500, checkpointing_steps=100000, resume_from_checkpoint=None, gradient_accumulation=4, gradient_checkpointing=None, lr=0.0001, scale_lr=None, scheduler='constant', warmup_steps=0, num_cycles=1, lr_power=1.0, dataloader_num_workers=0, use_8bit_adam=None, adam_beta1=0.9, adam_beta2=0.999, adam_weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, allow_tf32=None, prior_generation_precision=None, local_rank=-1, xformers=None, pre_compute_text_embeddings=None, tokenizer_max_length=None, text_encoder_use_attention_mask=None, rank=4, xl=None, fp16=True, bf16=None, hub_token=None, hub_model_id=None, push_to_hub=None, validation_prompt=None, num_validation_images=4, validation_epochs=50, checkpoints_total_limit=None, validation_images=None, logging=None, func=<function run_dreambooth_command_factory at 0x7ffa22b26320>)\u001b[0m\n",
            "> \u001b[1mINFO    Running DreamBooth Training\u001b[0m\n",
            "Downloading (…)okenizer_config.json: 100% 737/737 [00:00<00:00, 3.39MB/s]\n",
            "Downloading (…)tokenizer/vocab.json: 100% 1.06M/1.06M [00:00<00:00, 40.0MB/s]\n",
            "Downloading (…)tokenizer/merges.txt: 100% 525k/525k [00:00<00:00, 61.3MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 472/472 [00:00<00:00, 2.95MB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 725/725 [00:00<00:00, 4.12MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 460/460 [00:00<00:00, 2.84MB/s]\n",
            "Downloading (…)_encoder/config.json: 100% 565/565 [00:00<00:00, 2.87MB/s]\n",
            "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
            "Downloading (…)ncoder_2/config.json: 100% 575/575 [00:00<00:00, 3.42MB/s]\n",
            "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
            "Downloading model.safetensors: 100% 492M/492M [00:02<00:00, 174MB/s]\n",
            "Downloading model.safetensors: 100% 2.78G/2.78G [00:11<00:00, 248MB/s]\n",
            "Downloading (…)main/vae/config.json: 100% 607/607 [00:00<00:00, 3.31MB/s]\n",
            "Downloading (…)ch_model.safetensors: 100% 335M/335M [00:01<00:00, 302MB/s]\n",
            "Downloading (…)ain/unet/config.json: 100% 1.68k/1.68k [00:00<00:00, 10.1MB/s]\n",
            "Downloading (…)ch_model.safetensors: 100% 10.3G/10.3G [01:22<00:00, 125MB/s]\n",
            "Downloading (…)cheduler_config.json: 100% 479/479 [00:00<00:00, 1.81MB/s]\n",
            "{'dynamic_thresholding_ratio', 'clip_sample_range', 'variance_type', 'thresholding'} was not found in config. Values will be initialized to default values.\n",
            "> \u001b[1mINFO    Computing text embeddings for prompt: photo of sks person\u001b[0m\n",
            "> \u001b[1mINFO    ***** Running training *****\u001b[0m\n",
            "> \u001b[1mINFO      Num examples = 4\u001b[0m\n",
            "> \u001b[1mINFO      Num batches each epoch = 4\u001b[0m\n",
            "> \u001b[1mINFO      Num Epochs = 500\u001b[0m\n",
            "> \u001b[1mINFO      Instantaneous batch size per device = 1\u001b[0m\n",
            "> \u001b[1mINFO      Total train batch size (w. parallel, distributed & accumulation) = 4\u001b[0m\n",
            "> \u001b[1mINFO      Gradient Accumulation steps = 4\u001b[0m\n",
            "> \u001b[1mINFO      Total optimization steps = 500\u001b[0m\n",
            "> \u001b[1mINFO      Training config = model='stabilityai/stable-diffusion-xl-base-1.0' revision=None tokenizer=None image_path='images/' class_image_path=None prompt='photo of sks person' class_prompt=None num_class_images=100 class_labels_conditioning=None prior_preservation=False prior_loss_weight=1.0 output='output/' seed=42 resolution=512 center_crop=False train_text_encoder=False batch_size=1 sample_batch_size=4 epochs=500 num_steps=500 checkpointing_steps=100000 resume_from_checkpoint=None gradient_accumulation=4 gradient_checkpointing=False lr=0.0001 scale_lr=False scheduler='constant' warmup_steps=0 num_cycles=1 lr_power=1.0 dataloader_num_workers=0 use_8bit_adam=False adam_beta1=0.9 adam_beta2=0.999 adam_weight_decay=0.01 adam_epsilon=1e-08 max_grad_norm=1.0 allow_tf32=False prior_generation_precision=None local_rank=-1 xformers=False pre_compute_text_embeddings=False tokenizer_max_length=None text_encoder_use_attention_mask=False rank=4 xl=True fp16=True bf16=False hub_token=None hub_model_id=None push_to_hub=False validation_prompt=None num_validation_images=4 validation_epochs=50 checkpoints_total_limit=None validation_images=None logging=False\u001b[0m\n",
            "Steps: 100% 500/500 [31:21<00:00,  3.57s/it, loss=0.00461, lr=0.0001]Model weights saved in output/pytorch_lora_weights.safetensors\n",
            "Steps: 100% 500/500 [31:21<00:00,  3.76s/it, loss=0.00461, lr=0.0001]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zxD8E0nsbTQB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}